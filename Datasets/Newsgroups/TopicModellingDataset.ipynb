{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 8 - Topic Modelling using SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem Statement** - In this assignment, we work on 20 online newsgroups dataset. A newsgroup in a place on the Internet where people can ask questions related to certain topic. The data contains apprx 20k examples across 20 classes as shown below. The data is split into training at test sets as shown below. The dataset is available at http://qwone.com/~jason/20Newsgroups/ and is also available in sci-kit learn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 alt.atheism\n",
      "1 comp.graphics\n",
      "2 comp.os.ms-windows.misc\n",
      "3 comp.sys.ibm.pc.hardware\n",
      "4 comp.sys.mac.hardware\n",
      "5 comp.windows.x\n",
      "6 misc.forsale\n",
      "7 rec.autos\n",
      "8 rec.motorcycles\n",
      "9 rec.sport.baseball\n",
      "10 rec.sport.hockey\n",
      "11 sci.crypt\n",
      "12 sci.electronics\n",
      "13 sci.med\n",
      "14 sci.space\n",
      "15 soc.religion.christian\n",
      "16 talk.politics.guns\n",
      "17 talk.politics.mideast\n",
      "18 talk.politics.misc\n",
      "19 talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "obj = fetch_20newsgroups()\n",
    "#The dataset has the following classes, each class is mapped with an integer id.\n",
    "for i,j in enumerate(obj['target_names']):\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = fetch_20newsgroups(subset='train',random_state=10)\n",
    "data_test = fetch_20newsgroups(subset='test',random_state=10)\n",
    "\n",
    "X_train = data_train.data\n",
    "Y_train = data_train.target\n",
    "\n",
    "X_test = data_train.data\n",
    "Y_test = data_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: rj@ri.cadre.com (Rob deFriesse)\n",
      "Subject: Can DES code be shipped to Canada?\n",
      "Article-I.D.: fripp.1993Apr22.125402.27561\n",
      "Reply-To: rj@ri.cadre.com\n",
      "Organization: Cadre Technologies Inc.\n",
      "Lines: 13\n",
      "Nntp-Posting-Host: 192.9.200.19\n",
      "\n",
      "Someone in Canada asked me to send him some public domain DES file\n",
      "encryption code I have.  Is it legal for me to send it?\n",
      "\n",
      "Thanx.\n",
      "--\n",
      "Eschew Obfuscation\n",
      "\n",
      "Rob deFriesse                    Mail:  rj@ri.cadre.com\n",
      "Cadre Technologies Inc.          Phone:  (401) 351-5950\n",
      "222 Richmond St.                 Fax:    (401) 351-7380\n",
      "Providence, RI  02903\n",
      "\n",
      "I don't speak for my employer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "print(Y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Clearly we can see the class 11 is sci.crypt, which is evident from words like \"DES\",\"file\",\"encryption\",\"code\" in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tasks**\n",
    "1) Visualise a bar-chart chart, each bar showing the number of documents in each class.\n",
    "\n",
    "\n",
    "2) Data Cleaning  \n",
    "Perform Lemmatization and Stopword Removal on the dataset. Use WordNet Lemmatizer. Remove all numbers and non-english words like \"94\",\"a86\". Filter\n",
    "the top 500 words based upon the frequency to use as features.\n",
    "\n",
    "\n",
    "3) Use Count Vectorizer, or TF-IDF Vectorizer class to construct features.\n",
    "\n",
    "\n",
    "4) Use the following approaches(you can use the sci-kit version)\n",
    "- SVM with Linear Kernel and Grid Search over C = [1,2,5,10]\n",
    "- Report best score(Training and Testing Set) and best value of C. Use Cross Validation count cv=3.\n",
    "\n",
    "\n",
    "**Bonus**  \n",
    "Use K-Means(Unsupervised learning) to cluster similar documents together, all documents belonging to one topic can be clusterd together.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
