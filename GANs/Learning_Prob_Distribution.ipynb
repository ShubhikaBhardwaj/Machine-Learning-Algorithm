{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Learning Prob Distribution",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYP-8_MXGN6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGBmC3-MISkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Physics_std = 5\n",
        "Physics_mean = 80\n",
        "Physics = np.round(np.random.randn(500)*Physics_std + Physics_mean)\n",
        "\n",
        "Maths = np.round(np.random.randn(500)*10 + 40)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjcnWfSuI6b3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d0c94ccd-4e9e-4396-ae09-e4a7db3f1791"
      },
      "source": [
        "print(Physics)\n",
        "print(Maths)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[80. 86. 80. 84. 75. 77. 67. 82. 78. 81. 73. 83. 83. 81. 82. 81. 78. 80.\n",
            " 82. 78. 77. 75. 80. 83. 88. 76. 77. 77. 85. 87. 87. 87. 76. 76. 92. 68.\n",
            " 88. 87. 87. 82. 84. 82. 81. 86. 89. 76. 83. 81. 83. 81. 82. 76. 84. 73.\n",
            " 72. 85. 89. 79. 72. 79. 91. 78. 85. 89. 81. 83. 67. 81. 76. 79. 85. 79.\n",
            " 86. 72. 89. 68. 75. 80. 80. 72. 80. 78. 81. 75. 97. 78. 76. 75. 85. 74.\n",
            " 86. 74. 75. 80. 81. 83. 77. 72. 73. 82. 76. 80. 80. 79. 81. 80. 76. 77.\n",
            " 79. 78. 74. 84. 73. 86. 80. 81. 85. 84. 79. 78. 83. 75. 80. 79. 79. 82.\n",
            " 80. 84. 87. 78. 73. 81. 85. 81. 87. 77. 77. 77. 79. 73. 78. 71. 73. 79.\n",
            " 75. 82. 88. 81. 84. 81. 75. 73. 76. 78. 88. 76. 87. 84. 79. 76. 76. 70.\n",
            " 74. 87. 85. 85. 85. 87. 79. 85. 86. 84. 90. 90. 87. 83. 75. 83. 69. 82.\n",
            " 71. 78. 93. 83. 80. 75. 80. 80. 73. 79. 78. 77. 81. 88. 80. 91. 82. 90.\n",
            " 81. 90. 80. 74. 76. 76. 74. 79. 73. 81. 84. 83. 86. 80. 86. 85. 78. 78.\n",
            " 81. 85. 80. 77. 85. 73. 89. 71. 80. 80. 78. 84. 74. 70. 84. 73. 77. 77.\n",
            " 76. 80. 72. 84. 93. 73. 70. 77. 79. 77. 82. 69. 79. 90. 76. 85. 84. 83.\n",
            " 81. 87. 84. 86. 77. 76. 77. 90. 83. 87. 76. 83. 84. 88. 75. 88. 81. 84.\n",
            " 81. 77. 82. 68. 89. 74. 83. 79. 78. 88. 80. 78. 84. 80. 79. 84. 82. 78.\n",
            " 75. 77. 73. 75. 77. 84. 82. 74. 80. 69. 75. 82. 80. 82. 71. 76. 77. 76.\n",
            " 74. 81. 84. 73. 73. 83. 88. 80. 81. 82. 79. 81. 81. 84. 84. 78. 84. 82.\n",
            " 79. 78. 89. 76. 76. 76. 83. 81. 81. 83. 84. 89. 83. 78. 77. 87. 79. 76.\n",
            " 74. 83. 93. 83. 80. 81. 82. 73. 80. 83. 75. 77. 84. 76. 76. 80. 88. 79.\n",
            " 78. 73. 78. 77. 75. 85. 87. 84. 85. 76. 75. 73. 75. 80. 85. 84. 74. 67.\n",
            " 70. 80. 89. 81. 80. 78. 81. 77. 81. 77. 78. 79. 76. 73. 79. 75. 91. 84.\n",
            " 78. 83. 80. 93. 73. 88. 79. 79. 82. 81. 87. 75. 77. 84. 80. 75. 81. 84.\n",
            " 76. 81. 82. 76. 85. 82. 81. 80. 75. 74. 87. 76. 82. 84. 82. 82. 74. 69.\n",
            " 77. 77. 86. 78. 78. 78. 79. 81. 72. 80. 85. 81. 79. 73. 85. 85. 79. 83.\n",
            " 78. 85. 82. 85. 83. 81. 63. 80. 75. 92. 81. 71. 84. 88. 84. 76. 82. 78.\n",
            " 83. 77. 85. 78. 84. 83. 78. 77. 74. 90. 82. 84. 82. 90. 70. 78. 74. 78.\n",
            " 75. 78. 76. 73. 78. 76. 74. 68. 72. 72. 74. 76. 68. 78.]\n",
            "[47. 41. 58. 30. 42. 18. 53. 55. 40. 39. 39. 19. 56. 17. 48. 39. 50. 24.\n",
            " 44. 31. 24. 47. 37. 40. 39. 56. 33. 33. 30. 32. 34. 38. 57. 45. 33. 37.\n",
            " 55. 34. 18. 40. 55. 39. 46. 48. 46. 43. 36. 39. 36. 30. 34. 57. 46. 48.\n",
            " 37. 46. 48. 44. 27. 43. 26. 25. 51. 52. 26. 32. 46. 24. 39. 31. 26. 57.\n",
            " 26. 45. 41. 36. 46. 49. 50. 10. 44. 43. 39. 36. 51. 49. 36. 33. 39. 38.\n",
            " 27. 39. 21. 43. 38. 45. 43. 42. 37. 35. 41. 27. 19.  5. 42. 15. 34. 54.\n",
            " 35. 44. 44. 58. 48. 34. 36. 34. 28. 35. 47. 43. 40. 44. 33. 32. 35. 60.\n",
            " 63. 30. 47. 37. 29. 38. 66. 58. 48. 29. 37. 47. 37. 31. 50. 50. 30. 35.\n",
            " 29. 38. 37. 38. 47. 44. 27. 42. 37. 39. 58. 58. 55. 34. 37. 33. 54. 25.\n",
            " 51. 31. 37. 47. 31. 33. 43. 46. 29. 50. 55. 42. 39. 44. 36. 62. 42. 38.\n",
            " 39. 53. 35. 47. 26. 27. 51. 43. 51. 43. 47. 35. 46. 34. 49. 20. 41. 45.\n",
            " 26. 42. 47. 43. 44. 47. 39. 42. 54. 43. 33. 43. 40. 50. 35. 38. 33. 52.\n",
            " 24. 40. 50. 51. 41. 34. 51. 37. 44. 41. 27. 40. 41. 49. 41. 30. 66. 50.\n",
            " 24. 59. 56. 32. 49. 35. 32. 42. 34. 31. 37. 29. 44. 49. 30. 29. 40. 45.\n",
            " 49. 33. 17. 40. 41. 47. 50. 34. 25. 49. 31. 44. 50. 49. 45. 48. 43. 52.\n",
            " 42. 32. 38. 41. 38. 25. 35. 19. 37. 26. 40. 35. 44. 43. 49. 32. 35. 40.\n",
            " 45. 36. 34. 27. 36. 45. 52. 29. 47. 50. 45. 33. 59. 42. 26. 37. 42. 39.\n",
            " 46. 41. 30. 25. 24. 36. 26. 47. 51. 28. 48. 50. 48. 34. 46. 33. 20. 37.\n",
            " 42. 55. 26. 38. 22. 54. 26. 43. 14. 50. 35. 33. 39. 35. 24. 29. 34. 53.\n",
            " 37. 40. 42. 30. 44. 50. 32. 34. 35. 36. 19. 47. 35. 31. 46. 51. 32. 21.\n",
            " 48. 18. 36. 49. 14. 28. 39. 36. 31. 47. 23. 26. 54. 39. 24. 37. 16. 46.\n",
            " 55. 37. 47. 42. 13. 47. 41. 35. 44. 29. 47. 30. 42. 28. 39. 60. 53. 41.\n",
            " 29. 48. 32. 32. 51. 63. 38. 39. 48. 48. 48. 46. 54. 42. 33. 46. 45. 37.\n",
            " 54. 70. 46. 43. 33. 48. 43. 40. 37. 42. 43. 45. 39. 33. 39. 34. 51. 44.\n",
            " 28. 39. 53. 21. 32. 48. 53. 20. 25. 32. 41. 37. 33. 42. 36. 27. 34. 46.\n",
            " 38. 37. 39. 56. 35. 46. 34. 35. 35. 37. 49. 43. 39. 43. 39. 35. 32. 41.\n",
            " 58. 45. 60. 23. 35. 29. 58. 51. 55. 40. 33. 41. 47. 38. 52. 53. 39. 44.\n",
            " 38. 28. 34. 57. 27. 35. 43. 48. 27. 30. 44. 46. 46. 33.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ytp2cU-I7i0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "c38c21c8-f4ea-4a04-c7af-38bd9fbbdf27"
      },
      "source": [
        "plt.hist(Physics)\n",
        "plt.hist(Maths,alpha=0.7)\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAD8CAYAAACW/ATfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2wTz25AXGQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuqXQ30uJhn0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## GAN Training \n",
        "\n",
        "1. Generate a Mini Batch from Generator using Noise-Prior (Ground Truth - 0)\n",
        "2. Select a Mini Batch form Real Training Data (Ground Truth - 1)\n",
        "3. Train only Discriminator on both batches separately\n",
        "    - Consider Generator as Frozen\n",
        "    \n",
        "4. Train G using random noise as input and 1 as ground truth as 1 for all examples\n",
        "    - Consider Discriminator as Frozen\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFvbQ-BJZaHa",
        "colab_type": "text"
      },
      "source": [
        "## GAN Training \n",
        "\n",
        "**[GAN Paper](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf) Generative Adversarial Nets** \n",
        "Ian Goodfellow\n",
        "\n",
        "1. Generate a Mini Batch from Generator using Noise-Prior (Ground Truth - 0)\n",
        "2. Select a Mini Batch form Real Training Data (Ground Truth - 1)\n",
        "3. Train only Discriminator on both batches separately\n",
        "    - Consider Generator as Frozen\n",
        "    \n",
        "4. Train G using random noise as input and 1 as ground truth for all examples\n",
        "    - Consider Discriminator as Frozen\n",
        "\n",
        "\n",
        "## Training Tricks\n",
        "- Normalize inputs between [-1,1] and use tanh as activation for generator\n",
        "- Noise should be sample from Gaussian Distribtution\n",
        "- Avoid Sparse Gradients\n",
        "    - Prefer Leaky ReLU\n",
        "    - Prefer Strided Convolutions instead of max pooling\n",
        "    \n",
        " - Use Adam for optimization\n",
        " - One Sided Label Smoothing - Use 0 and 0.9 instead of 0 and 1 for discriminator\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b04WE9bLYzm9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rcgp3PJ_Yvg-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}